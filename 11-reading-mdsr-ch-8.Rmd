---
title: "11 - Reading MDSR Chapter 7"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# Chapter 7 - Statistical Foundations

```{r, message=FALSE}
library(mdsr)
library(tidyverse)
library(nycflights13)
```

# 7.1
## Example: Sampling from the population
A traveler has a meeting in San Francisco (SFO) at time **t**. Try to find how much earlier than **t** an acceptable flight shoudl arrive in order to avoid being late to the meeting due to a flight delay.

```{r}
View(flights)
str(flights)
```


Creating the population for this example - all flights that arrived in SFO in 2013.
```{r}
SF <- flights %>% 
  filter(dest == "SFO", !is.na(arr_delay))

View(SF)
```

Setting sample size to n = 25 cases.
```{r}
set.seed(101)
Sample25 <- SF %>% 
  sample_n(size = 25)
```

Looking at results from the sample
```{r}
favstats(data = Sample25, ~ arr_delay)
```

Comparing to population
```{r}
favstats(data = SF, ~ arr_delay)
```

98th percentile of the arrival delays in our data sample:
```{r}
qdata(~ arr_delay, p = 0.98, data = Sample25)
```

Testing to see if this policy would have worked by using the population data
```{r}
tally(~ arr_delay < 90, data = SF, format = "proportion")
```
This shows that our policy misses the mark 5% of the time - which is worse than intended. We need to increase it from 90 minutes but to what??

Since we have the population, you can calculate the 98th percentile of arrival delays
```{r}
qdata(~ arr_delay, p = 0.98, data = SF)
```
Now we know that it should have been about 150 minutes.

In the future when we don't have access to population, how can we find the 98th percentile with *only* the sample?


# 7.2 Sample Statistics
A more reliable sample statistic would be using the median. We ultimately want to know how well the sample statistic reflects the population.

## The sampling distribution
IF we draw many different samples from the population, each of size n, and calculated the sample statistic on each of those samples, how similar would the sample statistic be across all the samples?

```{r}
n <- 25
mean(~ arr_delay, data = sample_n(SF, size = n, replace = FALSE))
```

Now we need to repeat it multiple times.
```{r}
trials <- do(500) *
  mean(~ arr_delay, data = sample_n(SF, size = n, replace = FALSE))

head(trials)
```

Now that we have the average for 500 trails, let's examine how spread out the results are.
```{r}
favstats(~ mean, data = trials)
```
Here is the same thing with large sample size - n = 100.
```{r}
trials_100 <- do(500) *
  mean(~ arr_delay, data = SF %>% 
         sample_n(size = 100, replace = FALSE))
```

Histograms of both sample sizes
```{r}
rbind(trials %>% 
        mutate(n = 25),
      trials_100 %>% 
        mutate(n = 100)) %>% 
  ggplot(aes(x = mean)) +
  geom_histogram(bins = 30) +
  facet_grid(~ n) +
  xlab("Sample mean")
```

* Larger sample sizes produce a standard error that is smaller, and it is therefore more reliable.
* Larger sample sizes tend to result in a bell-shaped distribution