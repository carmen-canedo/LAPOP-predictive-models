---
title: "11 - Reading MDSR Chapter 7"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# Chapter 7 - Statistical Foundations

```{r, message=FALSE}
library(mdsr)
library(tidyverse)
library(nycflights13)
library(lubridate)
```

# 7.1
## Example: Sampling from the population
A traveler has a meeting in San Francisco (SFO) at time **t**. Try to find how much earlier than **t** an acceptable flight shoudl arrive in order to avoid being late to the meeting due to a flight delay.

```{r}
View(flights)
str(flights)
```


Creating the population for this example - all flights that arrived in SFO in 2013.
```{r}
SF <- flights %>% 
  filter(dest == "SFO", !is.na(arr_delay))

View(SF)
```

Setting sample size to n = 25 cases.
```{r}
set.seed(101)
Sample25 <- SF %>% 
  sample_n(size = 25)
```

Looking at results from the sample
```{r}
favstats(data = Sample25, ~ arr_delay)
```

Comparing to population
```{r}
favstats(data = SF, ~ arr_delay)
```

98th percentile of the arrival delays in our data sample:
```{r}
qdata(~ arr_delay, p = 0.98, data = Sample25)
```

Testing to see if this policy would have worked by using the population data
```{r}
tally(~ arr_delay < 90, data = SF, format = "proportion")
```
This shows that our policy misses the mark 5% of the time - which is worse than intended. We need to increase it from 90 minutes but to what??

Since we have the population, you can calculate the 98th percentile of arrival delays
```{r}
qdata(~ arr_delay, p = 0.98, data = SF)
```
Now we know that it should have been about 150 minutes.

In the future when we don't have access to population, how can we find the 98th percentile with *only* the sample?


# 7.2 Sample Statistics
A more reliable sample statistic would be using the median. We ultimately want to know how well the sample statistic reflects the population.

## The sampling distribution
IF we draw many different samples from the population, each of size n, and calculated the sample statistic on each of those samples, how similar would the sample statistic be across all the samples?

```{r}
n <- 25
mean(~ arr_delay, data = sample_n(SF, size = n, replace = FALSE))
```

Now we need to repeat it multiple times.
```{r}
trials <- do(500) *
  mean(~ arr_delay, data = sample_n(SF, size = n, replace = FALSE))

head(trials)
```

Now that we have the average for 500 trails, let's examine how spread out the results are.
```{r}
favstats(~ mean, data = trials)
```
Here is the same thing with large sample size - n = 100.
```{r}
trials_100 <- do(500) *
  mean(~ arr_delay, data = SF %>% 
         sample_n(size = 100, replace = FALSE))
```

Histograms of both sample sizes
```{r}
rbind(trials %>% 
        mutate(n = 25),
      trials_100 %>% 
        mutate(n = 100)) %>% 
  ggplot(aes(x = mean)) +
  geom_histogram(bins = 30) +
  facet_grid(~ n) +
  xlab("Sample mean")
```

* Larger sample sizes produce a standard error that is smaller, and it is therefore more reliable.
* Larger sample sizes tend to result in a bell-shaped distribution

# 7.3 The bootstrap
*The bootstrap* is a statistical method that allows us to approximate the sampling distribution even without access to the population.

With bootstrap, we think of the sample as the population. Through *resampling* we will draw a new sample from an existing sample but **with replacement**.

```{r}
small <- sample_n(SF, size = 3, replace = TRUE)
```

Let's use bootstrapping to find the reliability of the mean arrival time calculated on a sample size of 200. 
```{r}
n <- 200
original_sample <- SF %>% 
  sample_n(size = n, replace = FALSE)
```

From this sample, we'll draw all of the resamples and calculate the mean arrival delay.
```{r}
bootstrap_trials <- do(500) *
  mean(~ arr_delay, data = sample_n(original_sample, size = n, replace = TRUE))
favstats(~ mean, data = bootstrap_trials)
```

We can calculate the 98th percentile from our sample of size n = 100 flights, and use bootstrapping to see how reliable that sample statistic is.

```{r}
qdata(~ arr_delay, p = 0.98, data = original_sample)
```

We can check the reliability of that estimate using bootstrapping.
```{r}
bootstrap_trials <- do(500) * 
  qdata(~ arr_delay, p = 0.98,
        data = sample_n(original_sample, size = n, replace = TRUE))

favstats(~ quantile, data = bootstrap_trials)
```

The bootstrapped standard error is about 18 minutes. This would be unlikely to hit the target, and in this case even increaing the sample to 10,000 cases.

7.4 Outliers
One place where more data is helpful is in identifying outliers. Suppose we consider any flight delayed by 420 minutes (7 hours) or more as extreme.

```{r}
SF %>% 
  filter(arr_delay >= 420) %>% 
  select(month, day, dep_delay, arr_delay, carrier)
```

Most of the very long delays were in July, and Virgin America (VX) is the most frequent offender. Immediately this suggests one possible route for impoving the outcome of the business travel policy - we could tell people to arrive extra early in July and to avoid VX.

However, these outliers can be misleading, as they only account for a small fraction of flights into SFO in 2013.

*Outliers should never be dropped unless tehre is a clear rationale.*

*If outliers are dropped, this should be clearly reported.*

Histogram without the outliers.
```{r}
SF %>% 
  mutate(long_delay = arr_delay > 60) %>% 
  tally(~ long_delay | month, data = .)
```

The large majority of flights arrive without any delay or a delay less than 60 minutes. We might be able to identify patterns for when the longer delays are likely to occur.

The 14 outliers suggested that month or carrier may be linked to long delays. Let's see how that plays out with the large majority of data.

```{r}
SF %>% 
  mutate(long_delay = arr_delay > 60) %>% 
  tally(~ long_delay | month, data = .)
```

We can see that June and July are problem months.

```{r}
SF %>% 
  filter(arr_delay < 420) %>% 
  ggplot(aes(arr_delay)) +
  geom_histogram(binwidth = 15)
```

Distribution of flight arrival delays in 2013 for flights to SF from NYC airports that were delayed less than seven hours. The distribution features a long right tail (even after getting rid of the outliers).

```{r}
SF %>% 
  mutate(long_delay = arr_delay > 60) %>%
  tally(~ long_delay | carrier, data = .)
```

Delta Airines has reasonable performance. These analyses hint that a policy might advise travelers to plan to arrive extra early in June and July and to consider Delta as an airline for travel to SFO.

7.5 Statistical models: Explaining variation

In the previous section, we used month of the year and airline to narrow down the situations in which the risk of an unacceptable flight delay is large. Another way to think about this is that we are explaining part of the variation in arrival delay from flight to flight.

*Statistical modeling* provides a way to relate variables to one another. Doing so helps us better understand the system we are studying.

To illustrate modeling, let's consider another question from the airline delays dataset:

What impact, if any, does scheduled time of departure have on expected flight delay?

Many people think that earlier flights are less likely to delayed, since flight delays tend to cascade over the course of the day.

Is this theory supported by the data?

We first begin by considering the time of day. In the `nycflights13` package, the `flights` data frame has a variable (hour) that specifies the **scheduled** hour of departure.

```{r}
tally(~ hour, data = SF)
```

We see that many flights are scheduled in the early to mid-morning and from the late afternoon to early evening. None are scheduled before 5am or after 10pm.

Let's examine how the arrival delay depends on the house.

We'll do this in two ways:
1. Using standard box-and-whisker plots to show the distribution of arrival delays
2. Using a linear model that lets us track the mean arrival delay over the course of the day.

```{r}
SF %>% 
  ggplot(aes(x = hour, y = arr_delay)) +
  geom_boxplot(alpha = 0.1, aes(group = hour)) +
  geom_smooth(method = "lm") +
  xlab("Scheduled hour of departure") +
  ylab("Arrival delay (minutes)") +
  coord_cartesian(ylim = c(-30, 120))
```

The average arrival delay increases over the course of the day - this is a regression model.

```{r}
mod1 <- lm(arr_delay ~ hour, data = SF)
msummary(mod1)
```
Now we can build a model that includes variables we want to use to explain arrival delay.

The numbers in the "estimate" column for hour indicates that the arrival delay increases by 2 minutes every hour.

Standard error = 0.09 minutes per hour.

P-values < 0.05 mean that random, accidental patterns would be unlikely to generate an estimate as large as that observed. It rules out the possibility that the 2min/hr increase in arrival delay is just an accidental pattern.

What other factors could explain the delays?

Going to look at departure airport, carrier, month, and day. Also going to create variable season that summarizes what we already know about the month

These are *explanatory variables* to account for *response variables*.

```{r}
SF <- SF %>% 
  mutate(day = ymd(paste0(year, "-", month, "-", day)),
         dow = as.character(wday(day, label = TRUE)),
         season = ifelse(month %in% 6:7, "summer", "other month"))
```

Building model
```{r}
mod2 <- lm(arr_delay ~ hour +
             origin +
             carrier +
             season +
             dow,
             data = SF)

msummary(mod2)
```
The numbers in the "estimate" column tell us that we should add 4.2 minutes to the average delay if departing from JFK (instead of EWR - Newark).

Further analysis on pg. 162

*Liner models* describe how the mean of the response variable varies with the explanatory variables. This is the most widely used statistical model, but there are others!!

# 7.6 Confounding and accounting for other factors
When using observational data, it's important to check for confounding variables.

## Example
Using data on average teacher salaries and average total SAT scores for the 50 states, determine if higher teacher salaries are associated with better outcomes on the test at the state level. If so, should we adjust salaries to improve test performance?

```{r}
#SAT results by state in the year 2010
SAT_2010 <- SAT_2010 %>% 
  #Changing salary to manageable size
  mutate(Salary = salary/1000)

#Creating scatterplot of average SAT scores by average teacher salaries (in thousands of dollars)
sat_plot <- SAT_2010 %>% 
  ggplot(aes(x = Salary, y = total)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ylab("Average total score on the SAT") +
  xlab("Average teacher salary")

sat_plot
```


```{r}
#Making the linear model
sat_mod1 <- lm(total ~ Salary, data = SAT_2010)
msummary(sat_mod1)
```

```{r}
favstats(~ sat_pct, data = SAT_2010)
```

!! The percentage of students who take the SAT varies by 90% depending on the state.

Going to divide states into 2 groups: low and high.
```{r}
SAT_2010 <- SAT_2010 %>% 
  mutate(SAT_grp = ifelse(sat_pct <= 27, "Low", "High"))

tally(~ SAT_grp, data = SAT_2010)
```

Scatterplot of avg. SAT scores versus avg. teacher salaries (in thousands of dolalrs) for the 50 states in 2010, stratified by the percentage of students taking the SAT in each state.
```{r}
sat_plot %+% 
  SAT_2010 +
  aes(color = SAT_grp)
```

```{r}
coef(lm(total ~ Salary, data = filter(SAT_2010, SAT_grp == "Low")))

coef(lm(total ~ Salary, data = (filter(SAT_2010, SAT_grp == "High"))))
```
For each of the groups, the average teacher salary is positively associated with average SAT score, but when we collapse over this variable, avg. teacher salary is negatively associated with avg. SAT score. 

This form of confounding is a quantitative version of **Simpson's paradox**

1. Among states with a low percentage taking the SAT, teacher salaries and SAT scores are positively associated.

2. Among states with a high percentage taking the SAT, teacher salaries and SAT scores are positively associated. 

3. Among all states, salaries and SAT scores are negatively associated.

You can avoid this confounding by stratification (above) or multiple regression.

```{r}
sat_mod2 <- lm(total ~ Salary + sat_pct, data = SAT_2010)

msummary(sat_mod2)
```

The slope for salary is positive and statistically significant when we control for sat_pct, but we still can't really conclude that teacher salaries cause improvements in SAT scores. 

7.7 The perils of p-values

*p-value* is the probability of seeing a sample statistic as extreme (or more extreme) than the one that was observed if it were really the case that patterns in the data are a result of random change. 

*null hypothesis* assumes that all data is random.

For the SAT and salary example, the null hypothesis would be that the population regression coefficient  (slope) is 0.

When using *hypothesis testing* analysts declare results with a p-value of alpha = 0.05 or smaller *statistically significant*, and larger non-significant.