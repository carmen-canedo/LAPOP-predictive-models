---
title: "12 - Reading MDSR Chapter 8"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# Chapter 8
## Statistical learning and predictive analytics

Graphics work well when there are two or three variables involved. We will now be looking at models outside of a regression framework.

The idea that a general specification for a model could be turned to a specific data set automatically has led to the field of *machine learning*.

Two main branches in machine learning:
1. Supervised learning - modeling a specific response variable as a function of some explanatory variable
   + Data being studied already include measurements of outcome variables.

2. Unsupervised learning - approaches to finding patterns or groupings in data where there is not clear response variable
   + Outcome is unmeasured, task is often framed as a search for otherwise **unmeasured features** of the cases.
   
## 8.1 Supervised learning
The basic goal of supervised learning is to find a *function* that accurately describes how different measured explanatory variables can be combined to make a prediction about a response variable.

A function represents a relationship between inputs and an output.

`~` is used to define what the output variable (or variable on the left) is and what the input variables (or predictors on the right) are.

Expressions look like this:

`diabetic ~ age + sex + weight + height`

Here, the variable `diabetic` is marked as the output, simply because it is on the left side of `~`. The variables age, sex, weight, and height are to be the inputs to the function.

You can also see:

`diabetc ~ .`

The dot to the right means "use all the available variables (except the output)". The object above has class `formula`.

There are several different goals that might motivate constructing a function.
  + *Predict the output given an input.* It is February, what will the temperature be? Or on June 15th in Northampton, Massachusetts, U.S.A. (latitude 42.3 deg N), how many hours of daylight will there be?
  
  + *Determine which variables are useful inputs.* It is obvious from experience that temperature is a function of season. But in less familiar situations, e.g., predicting diabetes, the relevant inputs are uncertain or unknown.
  
  + *Generate hypotheses.* For a scientist trying to figure out the causes of diabetes, it can be useful to construct a predictive model, then look to see what variables turn out to be related to the risk of developing this disorder. 
    ++ You might find that diet, age, and blood pressure are risk factors. Socioeconomic status is not a direct cause of diabetes, but it might be that there is an association through factors related to the accessibility of health care. 
    ++ That is a **hypothesis**, and one that you probably would not have thought of before finding a function relating risk of diabetes to those inputs.
    
  + *Understand how a system works.* For example, a reasonable function relating hours of daylight to day-of-the-year and latitude reveals that the northern and southern hemisphere have reversed patterns: Long days in the southern hemisphere will be short days in the northern hemisphere.
  
Depending on your motivation, the kind of model and the input variables my differ.

In understanding how a system works, the variables you use should be related to the actual, causal mechanisms involved, e.g., the genetics of diabetes. For predicting an output, it hardly matters what the casual mechanisms are. Instead, all that's required is that the inputs are known at a time *before* the prediction is to be made. 

## 8.2 Classifiers
A logistic regression model takes a set of *explanatory variables* and converts them into a probability.

The analyst specifies the form of the relationship and what kind of variables are included.

*Classifers* serve as models for categorical response variables (see notes for mathematical explanation)

Classifiers are an important complement to regression models - while regression models have a **quantitative response variable** and can be visualized on a geometric surface, classification models have a **categorical response variable** and are often visualized as a discrete surface (i.e., a tree).

### 8.2.1 Decision trees
A decision tree assigns class labels to individual observations.

Each branch of the tree separates the records in the data set into increasingly "pure" (homogenous) subsets -- in the sense that they are *more likely to share the same class label.*

The number of possible decision tress grows exponentially with respect to the number of variables, p.

There isn't really an efficient algorithm to determine the optimal decision tree.

This book uses **recursive partitioning** decision trees using the package `rpart`.

The partitioning in a decision tree follow's Hunt's aglorithm, which is recursive.

Suppose we are somwhere in the decision tree