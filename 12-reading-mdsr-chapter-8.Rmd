---
title: "12 - Reading MDSR Chapter 8"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# Chapter 8
## Statistical learning and predictive analytics

Graphics work well when there are two or three variables involved. We will now be looking at models outside of a regression framework.

The idea that a general specification for a model could be turned to a specific data set automatically has led to the field of *machine learning*.

Two main branches in machine learning:
1. Supervised learning - modeling a specific response variable as a function of some explanatory variable
   + Data being studied already include measurements of outcome variables.

2. Unsupervised learning - approaches to finding patterns or groupings in data where there is not clear response variable
   + Outcome is unmeasured, task is often framed as a search for otherwise **unmeasured features** of the cases.
   
## 8.1 Supervised learning
The basic goal of supervised learning is to find a *function* that accurately describes how different measured explanatory variables can be combined to make a prediction about a response variable.

A function represents a relationship between inputs and an output.

`~` is used to define what the output variable (or variable on the left) is and what the input variables (or predictors on the right) are.

Expressions look like this:

`diabetic ~ age + sex + weight + height`

Here, the variable `diabetic` is marked as the output, simply because it is on the left side of `~`. The variables age, sex, weight, and height are to be the inputs to the function.

You can also see:

`diabetic ~ .`

The dot to the right means "use all the available variables (except the output)". The object above has class `formula`.

There are several different goals that might motivate constructing a function.
  + *Predict the output given an input.* It is February, what will the temperature be? Or on June 15th in Northampton, Massachusetts, U.S.A. (latitude 42.3 deg N), how many hours of daylight will there be?
  
  + *Determine which variables are useful inputs.* It is obvious from experience that temperature is a function of season. But in less familiar situations, e.g., predicting diabetes, the relevant inputs are uncertain or unknown.
  
  + *Generate hypotheses.* For a scientist trying to figure out the causes of diabetes, it can be useful to construct a predictive model, then look to see what variables turn out to be related to the risk of developing this disorder. 
    ++ You might find that diet, age, and blood pressure are risk factors. Socioeconomic status is not a direct cause of diabetes, but it might be that there is an association through factors related to the accessibility of health care. 
    ++ That is a **hypothesis**, and one that you probably would not have thought of before finding a function relating risk of diabetes to those inputs.
    
  + *Understand how a system works.* For example, a reasonable function relating hours of daylight to day-of-the-year and latitude reveals that the northern and southern hemisphere have reversed patterns: Long days in the southern hemisphere will be short days in the northern hemisphere.
  
Depending on your motivation, the kind of model and the input variables my differ.

In understanding how a system works, the variables you use should be related to the actual, causal mechanisms involved, e.g., the genetics of diabetes. For predicting an output, it hardly matters what the casual mechanisms are. Instead, all that's required is that the inputs are known at a time *before* the prediction is to be made. 

## 8.2 Classifiers
A logistic regression model takes a set of *explanatory variables* and converts them into a probability.

The analyst specifies the form of the relationship and what kind of variables are included.

*Classifers* serve as models for categorical response variables (see notes for mathematical explanation)

Classifiers are an important complement to regression models - while regression models have a **quantitative response variable** and can be visualized on a geometric surface, classification models have a **categorical response variable** and are often visualized as a discrete surface (i.e., a tree).

### 8.2.1 Decision trees
A decision tree assigns class labels to individual observations.

Each branch of the tree separates the records in the data set into increasingly "pure" (homogenous) subsets -- in the sense that they are *more likely to share the same class label.*

The number of possible decision tress grows exponentially with respect to the number of variables, p.

There isn't really an efficient algorithm to determine the optimal decision tree.

This book uses **recursive partitioning** decision trees using the package `rpart`.

The partitioning in a decision tree follow's Hunt's aglorithm, which is recursive.

A decision tree works by running this algorithm on the full training data set.

What does it mean to say that a set of records is "purer" than another set? Two popular methods for measuring purity of a set cadidate child nodes are the *Gini coefficient* and the *information gain*. Both are implemented in `rpart()`, which uses the Gini measurement by default.

### 8.2.2 Example: High-earners in the 1994 United States

A marketing analyst might be interested in finding factors that can be used to predict whether a potential customer is a high-earner. The 1994 United States Census provides information that can inform such a model, with records from 32,561 adults that include a binary variable indicating whether each person makes greater or less thatn $50,000. *This is our response variable*

```{r}
#Reading in data and saving to object
census <- read.csv(  "http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",  header = FALSE) 

#Adding column names
names(census) <- c("age", "workclass", "fnlwgt", "education",  "education.num", "marital.status", "occupation", "relationship",  "race", "sex", "capital.gain", "capital.loss", "hours.per.week",  "native.country", "income") 

glimpse(census)
```

First, we will separate our data set into two pieces by separating the rows at random.

A sample with 80% of the rows will become the **training** data set, and the remaining 20% will be the **testing** data set.

```{r}
#Setting the seed
set.seed(364)

#Assigning row length to object n
n <- nrow(census)

#Creating testing data set
test_idx <- sample.int(n, size = round(0.2 * n))

#Creating training data set
train <- census[-test_idx, ]

#Number of rows in training data set
nrow(train)

#Reassigning test data set
test <- census[test_idx, ]
nrow(test)
```

Note that only about 24% of those in the sample make more than $50k.

Thus, the **accuracy** of the **null model** is about 76% since we can get that many right by just predicting that everyone makes less than $50k.

```{r}
tally(~income, data = train, format = "percent")
```

```{r}
library(rpart)
rpart(income ~ capital.gain, data = train)
```

Although nearly 80% of those who paid less than 5095.5 dollars in capital gains tax made less than $50k, about 95% of those who paid **more**. Thus, partitioning the recods according to this criterion helps to divide them into relatively purer subsets.

```{r}
#Setting point of partition
split <- 5095.5

#Adding new column
train <- train %>% 
  mutate(hi_cap_gains = capital.gain >= split)

ggplot(data = train, aes(x = capital.gain, y = income)) +
  geom_count(aes(color = hi_cap_gains),
             position = position_jitter(width = 0, height = 0.1), alpha = 0.5) +
  geom_vline(xintercept = split, color = "dodgerblue", lty = 2) +
  scale_x_log10(labels = scales::dollar)
```
Thus, this decision tree uses a single variable (`capital.gains`) to partition the data set into two parts: those who paid more than 5095.5 dollars in capital gains, and those who did not.

For the former-who make up 0.951 of all observations-we get 79.4% right by predicting that they made less than $50k.

For the latter, we get 95% right by predicting that they made more than $50k.

Thus, our overall accuracy jumps to 80.1%, easily besting the 75.7% in the null model.

How did the algorithm now to pick $5095 as the threshold value?

It tried all of the sensible values, and this was the one that lowered the Gini coefficient the most.

So far we've only used one variable, but we can build a decision tree for income in terms of all of the other variables in the dataset.

```{r}
form <- as.formula("income ~ age + workclass + education + marital.status + occupation + relationship + race + sex + capital.gain + capital.loss + hours.per.week")

mod_tree <- rpart(form, data = train)
mod_tree
```

In this more complicated tree, the optimal first split now does not involve `capital.gain`, but rather `relationship`. 

A basic visualization of the tree can be created using the `plot()` function from `rpart` package.

```{r}
plot(mod_tree)
text(mod_tree, use.n = TRUE, all = TRUE, cex = 0.7)
```

That's ugly though, lol.

Instead, to make a nicer plot, use `partykit` - it has a series of functions for working with decision trees.

```{r}
library(partykit)
#Use show in new window to see clearly
plot(as.party(mod_tree))
```

This shows the *the decision tree itself*, whereas before it just shows how the tree recursively partitions the original data.

Here, the first question is whether `relationship` status is Husband or Wife. If not, then a capital gains threshold of $7,073.50 is used to determine one's income.

96.4% of those who paid more than the threshold earned more than $50k, but 94.9% of those who paid less than the threshold did not.

for those whose relationship status was Husband or Wife, the next question was whetehr you had a college degree. If so, then the model predicts with 72.8% accuracy that you made more than $50k.

If not, then again we ask about capital gains tax paid, but this time the threshold is 5095.50. 97.9% of those who were neither a husband nor a wife, and had no college degree, but paid more than that amount in capital gains tax, mae more than 50k. On the other hand, 70% of thosewho paid below the threshold made less than $50k.

```{r}
train <- train %>% 
  mutate(husband_or_wife = relationship %in% c("Husband", "Wife"),
         college_degree = husband_or_wife & education %in% c("Bachelors", "Doctorate", "Masters", "Prof-school"),
         income_dtree = predict(mod_tree, type = "class"))

cg_splits <- data.frame(husband_or_wife = c(TRUE, FALSE),
                        vals = c(5095.5, 7073.5))

ggplot(train, aes(x = capital.gain, y = income)) +
  geom_count(aes(color = income_dtree, shape = college_degree),
             position = position_jitter(width = 0, height = 0.1),
             alpha = 0.5) +
  facet_wrap(~ husband_or_wife) +
  geom_vline(data = cg_splits, aes(xintercept = vals),
             color = "dodgerblue", lty = 2) +
  scale_x_log10()
```

Since there are exponentially many tress, how did the algorithm know to pick this one?

*Complexity parameter* controls whether to keep or prune possible splits.

The algorithm considers many possible splits (i.e. new branches on the tree), but prunes them if they do not sufficiently improve the predictive power of the model (i.e. bear fruit). By default, each split has to decrease the error by a factor of 1%.

This will help to avoid *overfitting*.

Note that as we add more splits to our model, the relative error decreases.

```{r}
printcp(mod_tree)
```

An important tool in verifying a model's accuracy is called the *confusion matrix*.

This is a two-way table that counts how often our model made the correct prediction.

Note that there are two different types of mistakes that our model can make:
  + Predicting a high income when the income was in fact low
  + Predicting a low income when the income was in fact high
  
```{r}
train <- train %>% 
  mutate(income_dtree = predict(mod_tree, type = "class"))

confusion <- tally(income_dtree ~ income, data = train, format = "count")

confusion

sum(diag(confusion)) / nrow(train)
```
  
In this case, the accuracy of the decision tree classifier is now 84.6%, a considerable improvement over the null model.

### 8.2.3 Tuning parameters

The decision tree that we built above was based on the default parameters.

Most notably, our tree was pruned so that only splits that decreased the overall lack of fit by 1% were retained.

If we lower this threshold to 0.2%, then we get a more complex tree.

```{r}
mod_tree2 <- rpart(form, data = train, control = rpart.control(cp = 0.002))
```

Now we need to see if this is more or less accurate than our original tree.

```{r}
train2 <- train %>% 
  mutate(income_dtree = predict(mod_tree2, type = "class"))

confusion2 <- tally(income_dtree ~ income, data = train2, format = "count")

confusion2

sum(diag(confusion2)) / nrow(train)
```

This returns 86.3% accuracy, so it is less accurate than the original.

### 8.2.4 Random Forests

A natural extension of a decision tree is a *random forest* - a collection of decision trees that are aggregated by majority rule. 

#### Construction of a random forest:
1. Choosing the number of decision trees to grow (controlled by the `ntree` argument) and the number of variables to consider in each tree (`mtry`)

2. Randomly selecting the rows of the data frame **with replacement**

3. Randomly selecting `mtry` variables from the data frame

4. Buidling a decision tree on the resulting data set

5. Repeating this procedure `ntree` times

A prediction for a new observation is made by taking the majority rule from all of the decision trees in the forest.

```{r, message=FALSE}
library(randomForest)

mod_forest <- randomForest(form, data = train, ntree = 201, mtry = 3)
mod_forest
```

```{r}
sum(diag(mod_forest$confusion)) / nrow(train)
```

This means it is 86.6% accurate.

Because each tree in a random forest uses a different set of variables, it is possible to keep track of which variables seem to be the most consistently influential.

This is captured by the notion of *importance*.

Wihle there is no formal statistical inference here, importance can help to generate hypotheses.

Here, we see that `capital.gain` and `age` seem to be influential, while `race` and `sex` do not.

```{r}
library(tibble)

importance(mod_forest) %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  arrange(desc(MeanDecreaseGini))
```

A model object of class `randomForest` also has a `predict()` method for making new predictions.

### 8.2.5 and 8.2.6
Didn't seem to quite apply

### 8.2.7 Artificial neural networks
An *artificial neural network* is another classifier.

```{r}
library(nnet)

mod_nn <- nnet(form, data = train, size = 5)
```

A neural network is a directed graph that proceeds in stages.

First, there is one node for each input variables. In this case, because each factor level counts as its own variable, there are 57 input variables.

Next, there are a series of nodes specified as a **hidden** layer. In this case, we have specified five nodes for the hidden layer. Each of the input variables are connected to these hidden nodes. Each of the hidden nodes is connected to the single output variable. In addition, `nnet()` adds two control nodes, the first of which is connected to the five hidden nodes, and the latter is connected to the output noode.

The total number of edges is thus $ pk + k + k + 1 $, where k is the number of hidden nodes. In this case, there are $ 57 * 5 + 5 + 5 + 1 = 296 $ edges.

The algorithm iteratively searches for the optimal set of weights for each edge.

Once the weights are computed, the neural network can make predictions for new inputs by running these values through the network.

```{r}
income_nn <- predict(mod_nn, newdata = train, type = "class")

confusion <- tally(income_nn ~ income, data = train, format = "count")

confusion

sum(diag(confusion)) / nrow(train)
```

## 8.3 Ensemble methods
Not really sure about this one. Combe back to later? Seems more advanced.

## 8.4 Evaluating models

How do you know if your model is a good one?

This section outlines some of the concepts in model evaluation - a critical step in predictive analytics.

### 8.4.1 Cross-validation
One of the easiest traps to fall into is **overfitting**.

Every model discussed in this chapter is **fit** to a set of data. That is, given a set of **training** data and the specification for the type of model (e.g., decision tree, artifical neural network, etc.), each algorithm will determine the optimal set of parameters for the model and those data.

However, if the model works well on those training data, but not so well on a set of **testing** data - that the model has never seen - then the model is said to be *overfit*.

Perhaps the most elementary mistake in predicitive analytics is to overfit your model to the training data, only to see it later perform miserably on the testing set.

In predictive analytics, data sets are often divided into two sets:

*Training* The set of data on which you **build** your model

*Testing* Once your model is built, you test it by evaluating it against data that it has not previously seen.

For example, in this chapter we set aside 80% of the observations to use as a training set, but held back another 20% of testing.

The 80/20 scheme employed is one of the simplest possible, but there are many more complicated schemes.

Another approach to combat this problem is *cross-validation*.

To perform a 2-fold-cross-validation:
1. Randomly separate your data by rows into two data sets with the same number of observations. Let's call them X1 and X2.

2. Build your model on the data in X1, and then run the data in X2 through your model. How well does it perform? Just because your model performs well on X1 (**in-sample testing**), does not imply that it will perform as well on the data in X2 (**out-of-sample testing**)

3. Now reverse the roles of X1 and X2, so that the data in X2 is used for training, and the data in X1 is used for testing.

4. If your first model is overfit, then it will likely not perform as well on the second set of data.

### 8.4.2 Measuring prediction error
For evaluating models with a quantitative resposne, there are a variety of criteria that are commonly sused. There are three of the simplest and most common.

The following presumes a vector of real observations denoted y and a corresponding vector of prediction yhat:

*RMSE* Root mean squared error is probably the most common (there's an equation for it)

It is in the same units as the response variable  y, it captures both over and underestimates equally, and it penalizes large misses heavily.

*MAE* Mean absolute error is similar to the RMSE, but does not penalize large misses as heavily

*Correlation*

### 8.4.3 Confusion matrix
For classifiers, we have already seen the confusion matrix, which is a common way to asses the effectiveness of the model.

### 8.4.4 ROC Curves
Recall that each of the classifiers we have discussed are capable of producing not only a binary class label, but also the predicted probability of belonging to either class.

Rounding the probabilities the usual way (using 0.5 as a threshold) is not a good idea, since the average probability might not be anywhere near 0.5, and thus we could have far too many predictions in one class.

A principled way approach to assessing the quality of a classifier is a *receiver operating characteristic* curve.

This considers all possible threshold values for rounding, and graphically displays the trade-off between **sensitivity** (the true positive rate) and **specificity** (the true negative rate). 

What is actually plotted is the true postive rate as a function of the *false positive rate*. These can be constructed using `ROCR`. Note that ROC curves operate on the fitter probabilities in (0,1).

Review code when ready.

Going to start looking at extended example in a new notebook.